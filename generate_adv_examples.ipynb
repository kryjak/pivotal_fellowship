{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aUAUQc9lj_P"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes accelerate\n",
        "!pip list | grep transformers\n",
        "!pip list | grep bitsandbytes\n",
        "!pip list | grep accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9lzC4mUmw1j"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch as t\n",
        "from PIL import Image\n",
        "import requests"
      ],
      "metadata": {
        "id": "pDTkq8qx5XGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZkbyTGrqCfE"
      },
      "outputs": [],
      "source": [
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for storing params as 4-bit numbers\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=t.float16\n",
        ")"
      ],
      "metadata": {
        "id": "5pHB4JW16wv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGKsooWkkVaQ"
      },
      "outputs": [],
      "source": [
        "model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
        "\n",
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=t.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "# this is not needed here as BitsAndBytes automatically puts the model on device nad casts the correct dtype\n",
        "# model.to(device)\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
        "tokenizer = processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare image and text prompt, using the appropriate prompt template\n",
        "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n",
        "prompt_tokenized = tokenizer.encode(prompt)\n",
        "len_prompt_tokenized = len(prompt_tokenized)\n",
        "if type(prompt) == str:\n",
        "    batch_size = 1\n",
        "else:\n",
        "    batch_size = len(prompt)\n",
        "\n",
        "inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
        "target = \"The image appears to be a graphical representation of a machine learning model's performance on various tasks.\"\n",
        "\n",
        "target_tokenized = t.tensor(tokenizer.encode(target)).to(device)\n",
        "len_target_tokenized = len(target_tokenized)\n",
        "\n",
        "# farget target of random tokens, with '1' (BOS) as the first token\n",
        "# target_tokenized = t.cat((t.tensor([1]), t.randint(1, vocab_size, (len_target_tokenized-1, )))).to(device)\n",
        "\n",
        "# autoregressively complete prompt\n",
        "# 1. I don't get the difference between scores and logits:\n",
        "# t.allclose(output.scores[12], output.logits[12]) <-- True\n",
        "# https://github.com/huggingface/transformers/issues/14498#issuecomment-2110456886\n",
        "#\n",
        "# 2. The '-1' is because target_tokenized includes a BOS token, which we're not including in the slice below which is selecting the ASSISTANT's response\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=len_target_tokenized-1,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=False,\n",
        "    output_logits=True,\n",
        "    output_hidden_states=False,\n",
        "    output_attentions=False,\n",
        "    do_sample=False, # if False, it will use greedy encoding. Not sure if I should set it to True\n",
        "    use_cache=True # cache previous key and value vectors\n",
        "    )\n",
        "\n",
        "response_tokenized = output.sequences\n",
        "logits = t.stack(output.logits, dim=1) # [batch_size, seq_len, d_vocab]\n",
        "if batch_size == 1:\n",
        "    response_tokenized = response_tokenized.squeeze(0) # [seq_len]\n",
        "    logits = logits.squeeze(0) # [seq_len, d_vocab]\n",
        "else:\n",
        "    raise NotImplementedError('Batch size > 1 not implemented yet')\n",
        "\n",
        "response_tokenized = response_tokenized[len_prompt_tokenized:]\n",
        "response = tokenizer.decode(response_tokenized, skip_special_tokens=False)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "3um3CYVY6kmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_tokenized)\n",
        "print(target_tokenized)\n",
        "print(response_tokenized)"
      ],
      "metadata": {
        "id": "-jlXfJpTUADP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = t.nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, target_tokenized[1:])\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "Vow-liXFmxt1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}